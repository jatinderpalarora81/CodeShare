template = """
You are a Java code quality auditor with expertise in industry standards and best practices. Your task is to analyze the following code snippet in detail and provide suggestions to improve it across multiple aspects: security, performance, readability, and maintainability.

Code Context:
{context}

Code Snippet:
{code_snippet}

Instructions:
1. **Coding Standards**: Identify and correct any deviations from Java coding standards, including naming conventions and formatting.
2. **Security**: Check for vulnerabilities, such as SQL injection, insecure data handling, and other common security flaws.
3. **Performance**: Suggest optimizations for memory management, looping structures, and thread usage if applicable.
4. **Maintainability**: Recommend improvements for readability, modularity, and adherence to design patterns or architecture guidelines.
5. **Test Coverage**: Highlight areas that may need additional unit tests or edge case handling.
6. **Database Efficiency**: Review database interactions for query optimization, proper connection handling, and resource management.

For any issues found, provide suggestions to correct or improve them, adding comments directly in the code where appropriate.

Revised Code with Suggestions:
"""
----
template = """
You are a security auditor and code reviewer with expertise in Java and Shareworks standards. Analyze the following Java code snippet to determine whether it aligns with Shareworks SQLRunner guidelines.

Guidelines and Context:
{context}

Code Snippet:
{code_snippet}

Instructions:
1. **Compliance Check**: Check if the code follows the guidelines specified in the Shareworks SQLRunner document, especially the custom solution used to connect to the database and execute SQL queries.
2. **Identify Deviations**: Identify any parts of the code that deviate from these standards and document any potential security risks, such as SQL injection vulnerabilities, improper database handling, and other SQL-related issues.
3. **Suggested Changes**: For each identified deviation, provide specific suggestions to modify the code to align with Shareworks standards. Insert these suggestions as comments directly within the code for easy reference.

Return the code snippet with comments explaining any suggested changes.

Analysis:
"""
--------------
template = """
You are an expert in Java code security and a Shareworks project auditor. Your task is to review the following Java code to verify if it follows Shareworks-specific SQL handling standards as documented in the provided context.

Context (SQLRunner Document Details):
{context}

Java Code Snippet:
{code_snippet}

Instructions:
1. **Guideline Adherence**: Check if the code adheres to the SQL handling and database connection guidelines defined in the `SQLRunner` document. Ensure the code follows Shareworks’ custom standards, including methods for secure query execution and best practices for connection handling.
2. **SQL Injection and SQL-Related Issues**: Identify any potential SQL injection vulnerabilities or other SQL-related issues, such as improper input handling or risky query construction practices.
3. **Comments for Compliance and Recommendations**: For any code that does not follow the `SQLRunner` document guidelines, suggest changes to align with the documented standards. Add these suggestions directly as comments within the code.

Generate an updated code snippet that includes comments with improvement suggestions for non-compliant sections, highlighting potential SQL injection or other SQL-related issues.

Analysis with Suggested Improvements:
"""
-----------------
You are a code reviewer for the Shareworks project, with access to Shareworks-specific coding standards. 

Coding Standards:
{context}

Java Code Snippet:
{code_snippet}

Instructions:
1. **Compliance Check**: Analyze the provided Java code to determine if it follows the coding standards described in the context. 
2. **Highlight Non-Compliance**: Identify areas where the code does not adhere to the standards, noting any deviations.
3. **Add Comments with Suggestions**: For each identified issue, add a comment directly within the code suggesting how to align with the coding standards. 
4. **Format for Readability**: Place each suggestion on the line immediately above the non-compliant code, clearly explaining the change.

Output the modified code with comments indicating areas for improvement.

Review:
----------

Project Title: Intelligent Code & Document Analysis Using GenAI

Description: In this project, we aim to leverage the capabilities of Generative AI to create a tool that analyzes code and documentation to improve code quality, security, and maintainability. By scanning project documents (in text format) and dividing the codebase into manageable chunks, we plan to utilize the OpenAI API for comprehensive analysis based on the following key objectives:

Coding Standards Compliance: Check if the code adheres to project-specific coding styles and standards as defined in the provided documentation. This will help ensure consistency and adherence to best practices throughout the codebase.
Vulnerability and Security Analysis: Identify common security risks, including SQL injection, cross-site scripting, and other cyber vulnerabilities, enhancing the project's robustness against potential threats.
Memory Leak Detection: Analyze the code for potential memory leaks, providing insights into areas where memory management can be improved, particularly in resource-intensive modules.
Custom Solutions Identification: Detect and evaluate custom implementations within the code. For instance, our project has a custom JPA solution called SQLRunner. By supplying design and requirements documents related to SQLRunner as Retrieval-Augmented Generation (RAG) context, the tool will recognize instances and variations of SQLRunner in the code.
Design Pattern Analysis: Review the current use of design patterns within the code, identifying where established patterns are being utilized or where there are opportunities to implement them for better code structure and reusability.
Bug Detection: Use GenAI to scan for potential bugs based on established programming rules and contextual cues, facilitating proactive debugging and overall stability.
Impact: This project will yield a powerful GenAI-based code analysis tool that supports adherence to coding standards, enhances security, and promotes best practices in design patterns. It aims to help developers ensure a high-quality, maintainable, and secure codebase, driving efficiency and reliability in project development.

--------

Here’s a concise business-focused explanation for your hackathon idea:

"This solution enhances code quality and security by automatically ensuring adherence to coding standards, detecting vulnerabilities, and identifying memory leaks, thus reducing the risk of production issues.
By highlighting custom implementations and assessing design pattern usage, it optimizes maintainability and supports knowledge transfer.
Automated bug detection reduces manual review effort, enabling faster development cycles and improving developer productivity.
Real-time insights into security and code quality promote compliance and minimize potential cyber risks.
Overall, this innovation lowers operational costs and accelerates modernization by preemptively addressing technical debt."


------------               

Here are five performance-focused key performance indicators (KPIs) for your GenAI hackathon solution:

Code Compliance Accuracy: The precision in identifying adherence to coding standards and project-specific styles, benchmarked against project-defined guidelines.
Vulnerability Detection Rate: The effectiveness in catching cybersecurity issues, like SQL injection and cross-site scripting, measured by false positive and false negative rates.
Resource Leak Identification Efficiency: The solution's reliability in spotting potential memory leaks, impacting system performance and stability.
Custom Solution Analysis Relevance: The relevance and accuracy in recognizing custom implementations, such as SQLRunner, and validating them against provided requirements.
Pattern Recognition and Bug Detection Coverage: The comprehensiveness in detecting existing design patterns, suggesting improvements, and identifying potential bugs, with a focus on minimal oversight.
These KPIs focus on maintaining high detection accuracy, reducing false findings, and ensuring that critical issues are identified effectively.


----------
Here are two lines summarizing the impact:

"This solution will proactively enhance code quality and security by automating adherence to coding standards, identifying vulnerabilities, and ensuring design integrity."
"By leveraging GenAI for comprehensive code and document analysis, the project will streamline compliance, reduce potential risks, and improve maintainability across the codebase."
------

If you are using Gradle instead of Maven, the process of integrating svc2 into svc1 will follow similar steps, but with Gradle-specific configurations.

Let’s walk through the scenario where:

svc1 is a Spring Boot web application.
svc2 is a Spring Batch application, which you want to use as a dependency in svc1.
1. Packaging svc2 as a Dependency (JAR)
First, make sure svc2 is set up to be packaged as a JAR, so it can be added as a dependency in svc1.

Step 1: Configure svc2 as a Library in Gradle In the build.gradle file of svc2, ensure that the packaging type is set as a JAR. By default, Gradle creates a JAR for non-Spring Boot projects, but for Spring Boot projects, you might need to exclude the embedded server and configure the output properly.
Here’s how you can do it in svc2's build.gradle:

groovy
Copy code
plugins {
    id 'java'
    id 'org.springframework.boot' version '3.1.0'
    id 'io.spring.dependency-management' version '1.1.0'
}

group = 'com.example'
version = '1.0.0'

dependencies {
    implementation 'org.springframework.boot:spring-boot-starter-batch'
    implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
    runtimeOnly 'com.h2database:h2' // or other database
}

bootJar {
    enabled = false // This disables creating an executable JAR with embedded server
}

jar {
    enabled = true // This enables a regular JAR without Spring Boot's embedded server
}
Step 2: Publish svc2 to a Local or Remote Repository
Once svc2 is configured, you can build and publish the JAR. For local development, you can publish it to your local Maven repository.

In svc2’s project directory, run:

bash
Copy code
./gradlew publishToMavenLocal
This will create the JAR in your local Maven repository (typically ~/.m2/repository).
2. Adding svc2 as a Dependency in svc1
Once svc2 is published as a JAR, you can add it as a dependency in svc1 (the Spring Boot web service).

Step 3: Add svc2 as a Dependency in svc1's build.gradle
In svc1's build.gradle file, add the svc2 dependency:

groovy
Copy code
plugins {
    id 'java'
    id 'org.springframework.boot' version '3.1.0'
    id 'io.spring.dependency-management' version '1.1.0'
}

group = 'com.example'
version = '1.0.0'

repositories {
    mavenLocal()  // Ensure this is added to access your local repository
    mavenCentral()
}

dependencies {
    implementation 'com.example:svc2:1.0.0' // Dependency on svc2
    implementation 'org.springframework.boot:spring-boot-starter-web'
    implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
    runtimeOnly 'com.h2database:h2'
}
Now svc1 can access classes and beans from svc2.

3. Invoking svc2's Batch Jobs in svc1
Once svc2 is available as a dependency, you can invoke the batch jobs from svc1.

Step 4: Import svc2 Classes and Batch Jobs in svc1
In svc2, you might have a configuration class that defines your batch job, like this:

java
Copy code
// In svc2
@Configuration
@EnableBatchProcessing
public class BatchConfig {

    @Autowired
    private JobBuilderFactory jobBuilderFactory;

    @Autowired
    private StepBuilderFactory stepBuilderFactory;

    @Bean
    public Job sampleJob(Step step1) {
        return jobBuilderFactory.get("sampleJob")
                .incrementer(new RunIdIncrementer())
                .start(step1)
                .build();
    }

    @Bean
    public Step step1() {
        return stepBuilderFactory.get("step1")
                .tasklet((contribution, chunkContext) -> {
                    System.out.println("Running step 1 from svc2");
                    return RepeatStatus.FINISHED;
                })
                .build();
    }
}
In svc1, you can autowire this sampleJob and invoke it from a controller or a service.

Example:

java
Copy code
// In svc1
@RestController
public class BatchController {

    @Autowired
    private JobLauncher jobLauncher;

    @Autowired
    private Job sampleJob;  // Job from svc2

    @GetMapping("/runBatch")
    public String runBatchJob() {
        try {
            JobParameters jobParameters = new JobParametersBuilder()
                    .addLong("time", System.currentTimeMillis())
                    .toJobParameters();
            jobLauncher.run(sampleJob, jobParameters);
            return "Batch job has been invoked!";
        } catch (Exception e) {
            return "Error invoking batch job: " + e.getMessage();
        }
    }
}
4. Running svc1 and svc2 in Different JVMs (Optional)
If you want to run svc1 and svc2 in different JVMs, you can expose a REST API in svc2 to trigger the batch jobs and have svc1 call that API.

For example, in svc2, create a REST controller that triggers the batch job:

java
Copy code
// In svc2
@RestController
public class BatchTriggerController {

    @Autowired
    private JobLauncher jobLauncher;

    @Autowired
    private Job sampleJob;

    @GetMapping("/triggerBatch")
    public String triggerBatch() {
        try {
            JobParameters jobParameters = new JobParametersBuilder()
                    .addLong("time", System.currentTimeMillis())
                    .toJobParameters();
            jobLauncher.run(sampleJob, jobParameters);
            return "Batch job has been triggered!";
        } catch (Exception e) {
            return "Error triggering batch job: " + e.getMessage();
        }
    }
}
Then, in svc1, you can call the REST API of svc2:

java
Copy code
// In svc1
@RestController
public class RemoteBatchController {

    @GetMapping("/runRemoteBatch")
    public String runRemoteBatch() {
        RestTemplate restTemplate = new RestTemplate();
        String response = restTemplate.getForObject("http://localhost:8081/triggerBatch", String.class);
        return "Response from svc2: " + response;
    }
}
In this case, svc1 and svc2 are running in different JVMs, but svc1 triggers the batch job by making an HTTP request to svc2.

Summary with Gradle:
Package svc2 as a JAR and add it as a dependency in svc1.
Invoke svc2’s Spring Batch jobs directly from svc1 if running in the same JVM.
If running in separate JVMs, expose a REST endpoint in svc2 and invoke it from svc1.
Let me know if you'd like more examples or a different approach!



























How to Implement the Solution
Set Up the Shared Database for Spring Batch Metadata:

Ensure all Spring Batch jobs (from different JARs) point to the same Spring Batch metadata tables. This includes tables like BATCH_JOB_INSTANCE, BATCH_JOB_EXECUTION, BATCH_STEP_EXECUTION, etc.
Use a relational database like MySQL, PostgreSQL, or H2 (in production, use a production-grade DB like MySQL/Postgres).
Configure Each JAR to Use the Same Database:

In each JAR that runs a Spring Batch job, ensure the datasource points to the shared Spring Batch metadata tables.
Example application.properties for each Spring Batch JAR:

properties
Copy code
spring.datasource.url=jdbc:mysql://localhost:3306/spring_batch_db
spring.datasource.username=batch_user
spring.datasource.password=batch_password
spring.batch.initialize-schema=always
Job Operator Setup:

The JobOperator class allows you to control any job registered in the JobRepository, which means it can control jobs from multiple JARs if they share the same database.
You can create a JobOperator in a centralized controller application or even within one of the job JARs.
Example configuration:

java
Copy code
@Configuration
public class BatchConfig {

    @Bean
    public JobOperator jobOperator(JobExplorer jobExplorer, JobLauncher jobLauncher,
                                   JobRepository jobRepository, ListableJobLocator jobLocator) {
        SimpleJobOperator jobOperator = new SimpleJobOperator();
        jobOperator.setJobExplorer(jobExplorer);
        jobOperator.setJobLauncher(jobLauncher);
        jobOperator.setJobRepository(jobRepository);
        jobOperator.setJobRegistry(jobLocator);
        return jobOperator;
    }
}
Exposing Control APIs via a Centralized Interface:

You can use JobOperator in a separate centralized controller (e.g., a Spring Boot app) that exposes REST APIs for controlling the jobs (start, stop, restart, etc.).
This centralized application will use JobOperator to interact with the jobs across different JVMs because all the job executions are recorded in the shared metadata tables.
Example API to control jobs via JobOperator:

java
Copy code
@RestController
@RequestMapping("/job-control")
public class JobControlController {

    @Autowired
    private JobOperator jobOperator;

    @GetMapping("/start")
    public ResponseEntity<String> startJob(@RequestParam String jobName) {
        try {
            Long executionId = jobOperator.start(jobName, "time=" + System.currentTimeMillis());
            return ResponseEntity.ok("Job started with executionId: " + executionId);
        } catch (Exception e) {
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Job failed to start: " + e.getMessage());
        }
    }

    @GetMapping("/stop")
    public ResponseEntity<String> stopJob(@RequestParam Long executionId) {
        try {
            jobOperator.stop(executionId);
            return ResponseEntity.ok("Job stopped with executionId: " + executionId);
        } catch (Exception e) {
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Failed to stop job: " + e.getMessage());
        }
    }

    @GetMapping("/restart")
    public ResponseEntity<String> restartJob(@RequestParam Long executionId) {
        try {
            Long newExecutionId = jobOperator.restart(executionId);
            return ResponseEntity.ok("Job restarted with new executionId: " + newExecutionId);
        } catch (Exception e) {
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Failed to restart job: " + e.getMessage());
        }
    }

    @GetMapping("/get-status")
    public ResponseEntity<String> getJobStatus(@RequestParam Long executionId) {
        try {
            String status = jobOperator.getSummary(executionId);
            return ResponseEntity.ok("Job status: " + status);
        } catch (Exception e) {
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Failed to get job status: " + e.getMessage());
        }
    }
}
Deployment Architecture:

Multiple Job JARs: Each Spring Batch job can run in a separate JVM (or even on separate servers), but they all share the same Spring Batch metadata tables in a database.
Centralized Controller: You can have a separate Spring Boot application that serves as the controller for all jobs. This application will interact with JobOperator to start, stop, restart, or check the status of jobs.
Example of the deployment:

JAR 1 (Job 1): Deployed as one Spring Boot application with Spring Batch job and connected to the shared DB.
JAR 2 (Job 2): Another Spring Boot application with a different Spring Batch job, also connected to the same DB.
Controller Application: A separate Spring Boot application that uses JobOperator to control all jobs.
How Does the Common Interface Know About the Jobs?

JobOperator can query the shared Spring Batch tables to find out which jobs are running or have run. It can manage jobs by querying job names and execution statuses from the metadata.
For example, to get a list of running jobs, you can use JobExplorer:
java
Copy code
@Autowired
private JobExplorer jobExplorer;

@GetMapping("/running-jobs")
public ResponseEntity<List<Long>> getRunningJobs() {
    List<JobExecution> runningJobExecutions = jobExplorer.findRunningJobExecutions("yourJobName");
    List<Long> runningJobIds = runningJobExecutions.stream()
            .map(JobExecution::getId)
            .collect(Collectors.toList());
    return ResponseEntity.ok(runningJobIds);
}
Key Considerations
Concurrency and Locking:

Since multiple JARs will access the same Spring Batch tables, ensure your database has proper isolation levels and locking mechanisms to avoid concurrency issues.
Job Parameters:

Each job execution should have unique job parameters, or it will fail to start a new execution if one with the same parameters already exists. Use unique parameters (like timestamps) when starting jobs through the JobOperator.
Fault Tolerance:

Ensure your jobs handle failures gracefully. If one JVM goes down, the job metadata will still be in the database, and you can restart the job from a different JVM if necessary.
Clustered Environments:

If you want your jobs to run in a clustered environment, you can deploy the jobs in such a way that multiple instances can run the same job and be coordinated using Spring Batch's built-in mechanisms (like JobRepository).








-



This diagram shows:

A Job Manager Service which provides the interface for job control (start, stop, restart, etc.).
Multiple Spring Batch Job Applications (Job1, Job2, etc.) running in separate JVMs.
Each job exposes Listener APIs for various job lifecycle events (before/after job, step, chunk, etc.).
The Job Repository, where job execution and status are stored.
puml
Copy code
@startuml

!define RECTANGLE

title Generic API Interface for Spring Batch Jobs

'Legend'
legend right
  [C4 Level 2: System Container Diagram]
  - Job Manager Service exposes APIs to control batch jobs.
  - Each batch job (Job1, Job2, etc.) runs in a separate JVM.
  - The job control APIs interact with Spring Batch through JobOperator.
  - Listeners are exposed for job lifecycle events.
endlegend

'Spring Batch Architecture'
RECTANGLE JobManagerService as "Job Manager Service" <<Component>> {
  [start(String jobName, Properties parameters) : Long]
  [restart(long executionId) : Long]
  [stop(long executionId) : boolean]
  [getRunningExecutions(String jobName) : Set<Long>]
  [getJobInstances(String jobName, int start, int count) : List<Long>]
  [getSummary(long executionId) : String]
  [getJobNames() : Set<String>]
}

RECTANGLE SpringBatchJob1 as "Spring Batch Job1 (JVM1)" <<Container>> {
  [Job1Application] <<BatchJob>>
  [beforeJob(JobExecution jobExecution)]
  [afterJob(JobExecution jobExecution)]
  [beforeStep(StepExecution stepExecution)]
  [afterStep(StepExecution stepExecution)]
  [beforeChunk(ChunkContext chunkContext)]
  [afterChunk(ChunkContext chunkContext)]
  [beforeRead()]
  [afterRead(T item)]
}

RECTANGLE SpringBatchJob2 as "Spring Batch Job2 (JVM2)" <<Container>> {
  [Job2Application] <<BatchJob>>
  [beforeJob(JobExecution jobExecution)]
  [afterJob(JobExecution jobExecution)]
  [beforeStep(StepExecution stepExecution)]
  [afterStep(StepExecution stepExecution)]
  [beforeChunk(ChunkContext chunkContext)]
  [afterChunk(ChunkContext chunkContext)]
  [beforeRead()]
  [afterRead(T item)]
}

'Job Repository for storing job details and execution information'
RECTANGLE JobRepository as "Job Repository (Database)" <<Database>> {
  [job_execution]
  [step_execution]
  [job_instance]
}

JobManagerService -down- JobRepository : "Job execution control"
JobManagerService -down- SpringBatchJob1 : "API control & status"
JobManagerService -down- SpringBatchJob2 : "API control & status"
SpringBatchJob1 -down- JobRepository : "Read/Write Job Execution Data"
SpringBatchJob2 -down- JobRepository : "Read/Write Job Execution Data"

note right of JobManagerService
  Job Manager Service exposes:
  - Job control APIs (start, stop, restart, etc.)
  - Job status & summary retrieval (getRunningExecutions, getJobNames, getSummary)
  - Listener APIs for job lifecycle (beforeJob, afterJob, etc.)
end note

note right of SpringBatchJob1
  Job1 lifecycle listeners:
  - beforeJob, afterJob
  - beforeStep, afterStep
  - beforeChunk, afterChunk
  - beforeRead, afterRead
end note

note right of SpringBatchJob2
  Job2 lifecycle listeners:
  - beforeJob, afterJob
  - beforeStep, afterStep
  - beforeChunk, afterChunk
  - beforeRead, afterRead
end note

@enduml
Breakdown of Key Components:
Job Manager Service:

Exposes the generic API interface to control all jobs (start, stop, restart, etc.).
Implements methods like:
start(String jobName, Properties parameters)
restart(long executionId)
stop(long executionId)
getRunningExecutions(String jobName)
getJobNames(), etc.
Interacts with the Spring Batch JobOperator to manage jobs.
Spring Batch Job Applications:

Each job (Job1, Job2, etc.) is running as a standalone Spring Batch job in its own JVM.
Each job application exposes Listener APIs to monitor job lifecycle events, like:
beforeJob(JobExecution jobExecution)
afterJob(JobExecution jobExecution)
beforeStep(StepExecution stepExecution)
afterStep(StepExecution stepExecution)
beforeRead()
afterRead(T item)
beforeChunk(ChunkContext chunkContext)
afterChunk(ChunkContext chunkContext)
Job Repository:

The Job Repository (database) is used to store job instances, execution details, and status.
Both Job1 and Job2 interact with the job repository to update their execution status, parameters, and other metadata.
How the Diagram Depicts the Architecture:
The Job Manager Service provides an interface for job control and job status operations, while each job (Job1, Job2) runs in a separate JVM.
Each job has lifecycle listeners (beforeJob, afterJob, etc.) for monitoring different phases of the job execution.
All jobs interact with the Job Repository for reading and writing job execution data.
This UML diagram helps to visualize the architecture of a distributed Spring Batch job system with a central API controller.


import os
import csv
import shutil

def find_web_files(java_filename):
    # Replace .java with .html and .jsp in the file name
    html_filename = java_filename.replace(".java", ".html")
    jsp_filename = java_filename.replace(".java", ".jsp")
    return html_filename, jsp_filename

def search_web_files(directory, java_files):
    web_files = set()
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith((".html", ".jsp")):
                web_files.add(file)
    return web_files

def create_missing_files_csv(csv_file, code_directory, output_csv):
    # Read the CSV file to get the list of .java files
    with open(csv_file, 'r') as csvfile:
        reader = csv.reader(csvfile)
        java_files = [row[0] for row in reader]

    # Search for corresponding .html and .jsp files in the code base
    web_files = search_web_files(code_directory, java_files)

    # Find files with no corresponding .html or .jsp version
    missing_files = set(java_files) - web_files

    # Create a new CSV file with missing files
    with open(output_csv, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        for missing_file in missing_files:
            writer.writerow([missing_file])

    # Copy missing .java files to a new directory if needed
    for missing_file in missing_files:
        src_path = os.path.join(code_directory, missing_file)
        dest_path = os.path.join('new_files_directory', missing_file)
        shutil.copy2(src_path, dest_path)

if __name__ == "__main__":
    # Replace 'your_csv_file.csv', 'your_code_directory', and 'output_csv_file.csv' with actual paths
    csv_file_path = 'your_csv_file.csv'
    code_directory_path = 'your_code_directory'
    output_csv_path = 'output_csv_file.csv'

    create_missing_files_csv(csv_file_path, code_directory_path, output_csv_path)













from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.document_loaders import TextLoader
from langchain.chains import RetrievalQA,  ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.document_loaders import PyPDFLoader

def load_db(file, chain_type, k):
    # load documents
    loader = PyPDFLoader(file)
    documents = loader.load()
    # split documents
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    docs = text_splitter.split_documents(documents)
    # define embedding
    embeddings = OpenAIEmbeddings(openai_api_key='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')
    # create vector database from data
    db = DocArrayInMemorySearch.from_documents(docs, embeddings)
    # define retriever
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": k})
    # create a chatbot chain. Memory is managed externally.
    qa = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(model_name=llm_name, temperature=0, openai_api_key='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'), 
        chain_type=chain_type, 
        retriever=retriever, 
        return_source_documents=True,
        return_generated_question=True,
    )
    return qa 


import os
import openai
import sys
sys.path.append('../..')

import panel as pn  # GUI
pn.extension()

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.environ['LOGNAME']
print(openai.api_key )
# os.environ['OPENAI_API_KEY']
# OPENAI_API_KEY='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'


import datetime
current_date = datetime.datetime.now().date()
if current_date < datetime.date(2023, 9, 2):
    llm_name = "gpt-3.5-turbo-0301"
else:
    llm_name = "gpt-3.5-turbo"
print(llm_name)


import panel as pn
import param

class cbfs(param.Parameterized):
    chat_history = param.List([])
    answer = param.String("")
    db_query  = param.String("")
    db_response = param.List([])
    
    def __init__(self,  **params):
        super(cbfs, self).__init__( **params)
        self.panels = []
        self.loaded_file = "./b1.pdf"
        self.qa = load_db(self.loaded_file,"stuff", 4)
    
    def call_load_db(self, count):
        if count == 0 or file_input.value is None:  # init or no file specified :
            return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")
        else:
            file_input.save("temp.pdf")  # local copy
            self.loaded_file = file_input.filename
            button_load.button_style="outline"
            self.qa = load_db("temp.pdf", "stuff", 4)
            button_load.button_style="solid"
        self.clr_history()
        return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")

    def convchain(self, query):
        if not query:
            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown("", width=600)), scroll=True)
        result = self.qa({"question": query, "chat_history": self.chat_history})
        self.chat_history.extend([(query, result["answer"])])
        self.db_query = result["generated_question"]
        self.db_response = result["source_documents"]
        self.answer = result['answer'] 
        self.panels.extend([
            pn.Row('User:', pn.pane.Markdown(query, width=600)),
            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))
        ])
        inp.value = ''  #clears loading indicator when cleared
        return pn.WidgetBox(*self.panels,scroll=True)

    @param.depends('db_query ', )
    def get_lquest(self):
        if not self.db_query :
            return pn.Column(
                pn.Row(pn.pane.Markdown(f"Last question to DB:", styles={'background-color': '#F6F6F6'})),
                pn.Row(pn.pane.Str("no DB accesses so far"))
            )
        return pn.Column(
            pn.Row(pn.pane.Markdown(f"DB query:", styles={'background-color': '#F6F6F6'})),
            pn.pane.Str(self.db_query )
        )

    @param.depends('db_response', )
    def get_sources(self):
        if not self.db_response:
            return 
        rlist=[pn.Row(pn.pane.Markdown(f"Result of DB lookup:", styles={'background-color': '#F6F6F6'}))]
        for doc in self.db_response:
            rlist.append(pn.Row(pn.pane.Str(doc)))
        return pn.WidgetBox(*rlist, width=600, scroll=True)

    @param.depends('convchain', 'clr_history') 
    def get_chats(self):
        if not self.chat_history:
            return pn.WidgetBox(pn.Row(pn.pane.Str("No History Yet")), width=600, scroll=True)
        rlist=[pn.Row(pn.pane.Markdown(f"Current Chat History variable", styles={'background-color': '#F6F6F6'}))]
        for exchange in self.chat_history:
            rlist.append(pn.Row(pn.pane.Str(exchange)))
        return pn.WidgetBox(*rlist, width=600, scroll=True)

    def clr_history(self,count=0):
        self.chat_history = []
        return 


cb = cbfs()

file_input = pn.widgets.FileInput(accept='.pdf')
button_load = pn.widgets.Button(name="Load DB", button_type='primary')
button_clearhistory = pn.widgets.Button(name="Clear History", button_type='warning')
button_clearhistory.on_click(cb.clr_history)
inp = pn.widgets.TextInput( placeholder='Enter text here…')

bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)
conversation = pn.bind(cb.convchain, inp) 

jpg_pane = pn.pane.Image( './Downloads/marker23.jpg')

tab1 = pn.Column(
    pn.Row(inp),
    pn.layout.Divider(),
    pn.panel(conversation,  loading_indicator=True, height=300),
    pn.layout.Divider(),
)
tab2= pn.Column(
    pn.panel(cb.get_lquest),
    pn.layout.Divider(),
    pn.panel(cb.get_sources ),
)
tab3= pn.Column(
    pn.panel(cb.get_chats),
    pn.layout.Divider(),
)
tab4=pn.Column(
    pn.Row( file_input, button_load, bound_button_load),
    pn.Row( button_clearhistory, pn.pane.Markdown("Clears chat history. Can use to start a new topic" )),
    pn.layout.Divider(),
    pn.Row(jpg_pane.clone(width=400))
)
dashboard = pn.Column(
    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),
    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))
)
dashboard
